{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 120126,
          "databundleVersionId": 14369730,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31192,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "P0L7_v9IyBuG"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "shl_intern_hiring_assessment_2025_path = kagglehub.competition_download('shl-intern-hiring-assessment-2025')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "DyWpVerRyBuJ"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y numpy\n",
        "!pip install numpy==1.26.4\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T03:59:57.730792Z",
          "iopub.execute_input": "2025-12-17T03:59:57.73158Z",
          "iopub.status.idle": "2025-12-17T04:00:07.074866Z",
          "shell.execute_reply.started": "2025-12-17T03:59:57.731554Z",
          "shell.execute_reply": "2025-12-17T04:00:07.074119Z"
        },
        "id": "bg6HBpcnyBuK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Statement\n",
        "# GRAMMER SCORING ENGINE FOR SPOKEN DATA\n",
        "       \n",
        "The objective of this project is to develop a Grammar Scoring Engine that predicts a\n",
        "continuous grammar score (0â€“5) from spoken audio samples. Each audio file is 45â€“60\n",
        "seconds long, and the target labels are Mean Opinion Scores (MOS) based on a defined\n",
        "grammar rubric.\n",
        "\n",
        "This solution converts speech to text using an ASR model and evaluates grammatical\n",
        "quality using linguistic and syntactic features, followed by regression-based modeling."
      ],
      "metadata": {
        "id": "z98wN5hcyBuL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data loading  & Exploration\n",
        "* Training sample: 409\n",
        "* Test sample : 197\n",
        "* Each audio file : 45-60 second\n",
        "* Labels : Grammar MOS scores (0-5)"
      ],
      "metadata": {
        "id": "MNIUCPQPyBuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "train_csv_path = os.path.join(shl_intern_hiring_assessment_2025_path, \"dataset\", \"csvs\", \"train.csv\")\n",
        "test_csv_path = os.path.join(shl_intern_hiring_assessment_2025_path, \"dataset\", \"csvs\", \"test.csv\")\n",
        "\n",
        "train_df = pd.read_csv(train_csv_path)\n",
        "test_df = pd.read_csv(test_csv_path)\n",
        "\n",
        "print(\"train Data \", train_df.head(10))\n",
        "print(\"Test Data\", test_df.head())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T04:00:10.709411Z",
          "iopub.execute_input": "2025-12-17T04:00:10.709769Z",
          "iopub.status.idle": "2025-12-17T04:00:10.755273Z",
          "shell.execute_reply.started": "2025-12-17T04:00:10.709751Z",
          "shell.execute_reply": "2025-12-17T04:00:10.754742Z"
        },
        "id": "mwrwIuFFyBuO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check column"
      ],
      "metadata": {
        "id": "deldkk_ayBuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.columns"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T04:00:10.75694Z",
          "iopub.execute_input": "2025-12-17T04:00:10.757131Z",
          "iopub.status.idle": "2025-12-17T04:00:10.763928Z",
          "shell.execute_reply.started": "2025-12-17T04:00:10.757115Z",
          "shell.execute_reply": "2025-12-17T04:00:10.763118Z"
        },
        "id": "fd3gA_5jyBuP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['label'].describe()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T04:00:10.764824Z",
          "iopub.execute_input": "2025-12-17T04:00:10.765118Z",
          "iopub.status.idle": "2025-12-17T04:00:10.783888Z",
          "shell.execute_reply.started": "2025-12-17T04:00:10.765093Z",
          "shell.execute_reply": "2025-12-17T04:00:10.783149Z"
        },
        "id": "oK6yIUmsyBuQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualisation"
      ],
      "metadata": {
        "id": "Ki6hnOR5yBuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "\n",
        "plt.hist(train_df['label'], bins=10)\n",
        "plt.xlabel(\"Grammer Score\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Grammer Score Distribution\")\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T04:00:10.784684Z",
          "iopub.execute_input": "2025-12-17T04:00:10.784904Z",
          "iopub.status.idle": "2025-12-17T04:00:11.793746Z",
          "shell.execute_reply.started": "2025-12-17T04:00:10.784874Z",
          "shell.execute_reply": "2025-12-17T04:00:11.79316Z"
        },
        "id": "dJT-3swSyBuS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Constant"
      ],
      "metadata": {
        "id": "KZmZDSAwyBuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Correct audio directory (dataset folder uses 'audios', not 'audio_files')\n",
        "audio_dir =  os.path.join(shl_intern_hiring_assessment_2025_path , \"dataset\", \"audios\", \"train\")\n",
        "File_name_col = \"filename\"\n",
        "label_col = \"label\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T04:00:11.794489Z",
          "iopub.execute_input": "2025-12-17T04:00:11.794827Z",
          "iopub.status.idle": "2025-12-17T04:00:11.798362Z",
          "shell.execute_reply.started": "2025-12-17T04:00:11.794808Z",
          "shell.execute_reply": "2025-12-17T04:00:11.797721Z"
        },
        "id": "R7lOoQp4yBuT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Verify Audio Path"
      ],
      "metadata": {
        "id": "YrIjDkZqyBuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a robust filename from the CSV value\n",
        "raw_name = train_df.iloc[0].get(File_name_col, '')\n",
        "raw_name = str(raw_name).strip()\n",
        "if raw_name == '' or raw_name.lower() == 'nan':\n",
        "    raise ValueError(f'Filename column is empty for first row: {raw_name}')\n",
        "sample_file = raw_name if raw_name.lower().endswith('.wav') else raw_name + '.wav'\n",
        "audio_path = os.path.join(audio_dir, sample_file)\n",
        "\n",
        "# Diagnostics: show resolved paths and directory contents\n",
        "print('Audio dir (raw):', audio_dir)\n",
        "print('Audio dir (abs):', os.path.abspath(audio_dir))\n",
        "try:\n",
        "    files = sorted(os.listdir(audio_dir))\n",
        "    print('Number of files in audio_dir:', len(files))\n",
        "    print('First 40 files:', files[:40])\n",
        "    print('sample_file in dir?:', sample_file in files)\n",
        "except FileNotFoundError as e:\n",
        "    print('Directory not found:', e)\n",
        "\n",
        "print(f'CSV filename value: {raw_name}')\n",
        "print('Audio filename used:', sample_file)\n",
        "print('Full path:', audio_path)\n",
        "print('Exists:', os.path.exists(audio_path))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T04:00:11.799259Z",
          "iopub.execute_input": "2025-12-17T04:00:11.799593Z",
          "iopub.status.idle": "2025-12-17T04:00:11.815039Z",
          "shell.execute_reply.started": "2025-12-17T04:00:11.79957Z",
          "shell.execute_reply": "2025-12-17T04:00:11.814519Z"
        },
        "id": "qPlRHvyayBuU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Check all audio  Validation"
      ],
      "metadata": {
        "id": "uTXRnN4hyBuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check missing files robustly: append .wav when necessary and show diagnostics\n",
        "missing = []\n",
        "csv_count = len(train_df)\n",
        "for fname in train_df['filename']:\n",
        "    name = str(fname).strip()\n",
        "    if name == '' or name.lower() == 'nan':\n",
        "        continue\n",
        "    sample = name if name.lower().endswith('.wav') else name + '.wav'\n",
        "    path = os.path.join(audio_dir, sample)\n",
        "    if not os.path.exists(path):\n",
        "        missing.append(sample)\n",
        "\n",
        "# Diagnostics summary\n",
        "try:\n",
        "    files = sorted(os.listdir(audio_dir))\n",
        "except FileNotFoundError:\n",
        "    files = []\n",
        "print('CSV rows:', csv_count)\n",
        "print('Files in audio_dir:', len(files))\n",
        "print('Missing files (count):', len(missing))\n",
        "# Show a sample of missing filenames\n",
        "missing[:50]\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T04:00:11.815668Z",
          "iopub.execute_input": "2025-12-17T04:00:11.815902Z",
          "iopub.status.idle": "2025-12-17T04:00:12.18668Z",
          "shell.execute_reply.started": "2025-12-17T04:00:11.815867Z",
          "shell.execute_reply": "2025-12-17T04:00:12.186137Z"
        },
        "id": "0oe4j-LayBuU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# preprocessing & Pipeline Architecture\n",
        "\n",
        "### Pipline Overview:\n",
        "Audio-> Preprocessing->Speech-to-text-> text Cleaning -> Grammer Feature Extraction ->  Regression Model -> Score (0-5)\n",
        "\n",
        "### Audio Preprocessing:\n",
        "* Resampling to 16hz\n",
        "* Silence trimming\n",
        "* Loudness normalization\n",
        "### Speech-to-text:\n",
        "* OpenAI Whisper ASR\n",
        "* English Only Transcription\n",
        "### Feature Engineering :\n",
        "* Grammer error counts\n",
        "* Sentence statics\n",
        "* POS ratios\n",
        "* Syntactic tree depth\n"
      ],
      "metadata": {
        "id": "L_jnC7hnyBuU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Load Require Libraries"
      ],
      "metadata": {
        "id": "DqEFMt9zyBuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import librosa"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T04:00:12.188743Z",
          "iopub.execute_input": "2025-12-17T04:00:12.188939Z",
          "iopub.status.idle": "2025-12-17T04:00:12.197072Z",
          "shell.execute_reply.started": "2025-12-17T04:00:12.188924Z",
          "shell.execute_reply": "2025-12-17T04:00:12.196495Z"
        },
        "id": "u5r8acLayBuV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Audio Preprocessing Function"
      ],
      "metadata": {
        "id": "eKhd-fpzyBuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def preprocesss_audio(audio_path, target_sr=16000, max_duration=60):\n",
        "    # load _audio\n",
        "    y, sr =  librosa.load(audio_path, sr=target_sr)\n",
        "    y , _ = librosa.effects.trim(y, top_db=20)\n",
        "\n",
        "    # normalize\n",
        "    max_len = sr * max_duration\n",
        "    if len(y)>max_len:\n",
        "        y = y[:max_len]\n",
        "    else:\n",
        "        y = np.pad(y, (0, max_len - len(y)))\n",
        "\n",
        "\n",
        "    return y"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T04:00:12.197785Z",
          "iopub.execute_input": "2025-12-17T04:00:12.197971Z",
          "iopub.status.idle": "2025-12-17T04:00:12.211671Z",
          "shell.execute_reply.started": "2025-12-17T04:00:12.197957Z",
          "shell.execute_reply": "2025-12-17T04:00:12.210899Z"
        },
        "id": "goytD029yBuV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Preprocessing on One File"
      ],
      "metadata": {
        "id": "R4CeR1-EyBuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_wav =  train_df.iloc[0]['filename'].strip()\n",
        "sample = sample_wav if sample_wav.lower().endswith('.wav') else sample_wav + '.wav'\n",
        "Audio_path =  os.path.join(audio_dir, sample)\n",
        "\n",
        "y = preprocesss_audio(audio_path=Audio_path)\n",
        "print(\"Processed duration (sec):\", len(y)/16000)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T04:00:12.212606Z",
          "iopub.execute_input": "2025-12-17T04:00:12.21278Z",
          "iopub.status.idle": "2025-12-17T04:00:26.209554Z",
          "shell.execute_reply.started": "2025-12-17T04:00:12.212767Z",
          "shell.execute_reply": "2025-12-17T04:00:26.208771Z"
        },
        "id": "26zeuutuyBuV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Listen sample preprocess audio"
      ],
      "metadata": {
        "id": "ZgXx_UfqyBuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython.display as ipd\n",
        "\n",
        "\n",
        "print(\"Processed audio:\")\n",
        "ipd.Audio(y, rate=16000)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T04:00:26.210377Z",
          "iopub.execute_input": "2025-12-17T04:00:26.211016Z",
          "iopub.status.idle": "2025-12-17T04:00:26.265347Z",
          "shell.execute_reply.started": "2025-12-17T04:00:26.210996Z",
          "shell.execute_reply": "2025-12-17T04:00:26.264634Z"
        },
        "id": "OQjp5TT0yBuW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Whisper"
      ],
      "metadata": {
        "id": "dtVcRWf9yBuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U openai-whisper\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T04:00:26.266241Z",
          "iopub.execute_input": "2025-12-17T04:00:26.266506Z",
          "iopub.status.idle": "2025-12-17T04:01:44.462591Z",
          "shell.execute_reply.started": "2025-12-17T04:00:26.266485Z",
          "shell.execute_reply": "2025-12-17T04:01:44.461738Z"
        },
        "id": "qwC_yNtsyBuW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Whisper Model"
      ],
      "metadata": {
        "id": "hSUEGsyAyBuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "whisper_model = whisper.load_model(\"base\")\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T04:01:44.46368Z",
          "iopub.execute_input": "2025-12-17T04:01:44.463965Z",
          "iopub.status.idle": "2025-12-17T04:01:50.563063Z",
          "shell.execute_reply.started": "2025-12-17T04:01:44.463931Z",
          "shell.execute_reply": "2025-12-17T04:01:50.562442Z"
        },
        "id": "R-Jw72hPyBuW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transcript Function"
      ],
      "metadata": {
        "id": "DCkWlTUlyBuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transcribe_audio(audio_path):\n",
        "    result = whisper_model.transcribe(\n",
        "        audio_path,\n",
        "        language=\"en\",\n",
        "        fp16=False\n",
        "    )\n",
        "    return result[\"text\"]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T04:01:50.563919Z",
          "iopub.execute_input": "2025-12-17T04:01:50.564311Z",
          "iopub.status.idle": "2025-12-17T04:01:50.568149Z",
          "shell.execute_reply.started": "2025-12-17T04:01:50.564292Z",
          "shell.execute_reply": "2025-12-17T04:01:50.567416Z"
        },
        "id": "N-qz9hNTyBuX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test on one audio file"
      ],
      "metadata": {
        "id": "mLlTiTbSyBuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_file = train_df.iloc[0]['filename']\n",
        "sample = sample_file if sample_file.lower().endswith('.wav') else sample_file + '.wav'\n",
        "audio_path = os.path.join(audio_dir, sample)\n",
        "\n",
        "text = transcribe_audio(audio_path)\n",
        "\n",
        "print(\"TRANSCRIBED TEXT:\\n\")\n",
        "print(text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T04:01:50.568837Z",
          "iopub.execute_input": "2025-12-17T04:01:50.569013Z",
          "iopub.status.idle": "2025-12-17T04:01:52.639934Z",
          "shell.execute_reply.started": "2025-12-17T04:01:50.568999Z",
          "shell.execute_reply": "2025-12-17T04:01:52.639235Z"
        },
        "id": "2S1ZNoLXyBuX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean Transcribe Text function"
      ],
      "metadata": {
        "id": "vmMGz4BEyBuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def clean_text(text):\n",
        "    text =  text.lower()\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text.strip()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T04:01:52.640829Z",
          "iopub.execute_input": "2025-12-17T04:01:52.641118Z",
          "iopub.status.idle": "2025-12-17T04:01:52.645578Z",
          "shell.execute_reply.started": "2025-12-17T04:01:52.641094Z",
          "shell.execute_reply": "2025-12-17T04:01:52.644814Z"
        },
        "id": "U38426FTyBuX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "FjhhztHsyBuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text = clean_text(text)\n",
        "print(cleaned_text)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T04:01:52.646332Z",
          "iopub.execute_input": "2025-12-17T04:01:52.646679Z",
          "iopub.status.idle": "2025-12-17T04:01:52.660369Z",
          "shell.execute_reply.started": "2025-12-17T04:01:52.646655Z",
          "shell.execute_reply": "2025-12-17T04:01:52.659783Z"
        },
        "id": "bUEgCDTJyBuY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transcribe Full Train Set"
      ],
      "metadata": {
        "id": "MPN5K6zKyBui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "train_df[\"transcript\"] = \"\"\n",
        "\n",
        "for i in tqdm(range(len(train_df))):\n",
        "    file_name = train_df.loc[i, \"filename\"]\n",
        "    file =  file_name if file_name.lower().endswith('.wav') else file_name + '.wav'\n",
        "    audio_path = os.path.join(audio_dir, file)\n",
        "\n",
        "    text = transcribe_audio(audio_path)\n",
        "    train_df.loc[i, \"transcript\"] = clean_text(text)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T04:01:52.66097Z",
          "iopub.execute_input": "2025-12-17T04:01:52.661145Z",
          "iopub.status.idle": "2025-12-17T04:09:54.207144Z",
          "shell.execute_reply.started": "2025-12-17T04:01:52.661128Z",
          "shell.execute_reply": "2025-12-17T04:09:54.206496Z"
        },
        "id": "OMehfJzWyBuj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save the transcribe"
      ],
      "metadata": {
        "id": "3ZymuhqFyBuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.to_csv(\"train_with_transcripts.csv\", index=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T04:09:54.208039Z",
          "iopub.execute_input": "2025-12-17T04:09:54.208329Z",
          "iopub.status.idle": "2025-12-17T04:09:54.224363Z",
          "shell.execute_reply.started": "2025-12-17T04:09:54.208304Z",
          "shell.execute_reply": "2025-12-17T04:09:54.223784Z"
        },
        "id": "AL2DPn_iyBuj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For Check"
      ],
      "metadata": {
        "id": "7BZIKb5zyBuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[[\"filename\", \"label\", \"transcript\"]].head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T04:09:54.225094Z",
          "iopub.execute_input": "2025-12-17T04:09:54.225417Z",
          "iopub.status.idle": "2025-12-17T04:09:54.241964Z",
          "shell.execute_reply.started": "2025-12-17T04:09:54.225375Z",
          "shell.execute_reply": "2025-12-17T04:09:54.241269Z"
        },
        "id": "EHkA1g7GyBuk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grammer Feature Engineering\n",
        "\n",
        "we extract explainable linguistic feature such as:\n",
        "* Word count\n",
        "* Sentence  count\n",
        "* Grammar Errors per sentence\n",
        "* POS ratios (noun , verb , adjective, adverb)\n",
        "* Average depandency tree depth"
      ],
      "metadata": {
        "id": "EYDSoA_TyBuk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Required Libraries"
      ],
      "metadata": {
        "id": "LJT7kmfLyBuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install language-tool-python spacy\n",
        "!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T04:09:54.243296Z",
          "iopub.execute_input": "2025-12-17T04:09:54.243765Z",
          "iopub.status.idle": "2025-12-17T04:10:10.785103Z",
          "shell.execute_reply.started": "2025-12-17T04:09:54.243742Z",
          "shell.execute_reply": "2025-12-17T04:10:10.783854Z"
        },
        "id": "2jrzXuxYyBuk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Tools"
      ],
      "metadata": {
        "id": "5C_5JLACyBul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import language_tool_python\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "tool = language_tool_python.LanguageTool(\n",
        "    'en-US',\n",
        "    remote_server='https://api.languagetool.org'\n",
        ")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T04:10:10.786366Z",
          "iopub.execute_input": "2025-12-17T04:10:10.786657Z",
          "iopub.status.idle": "2025-12-17T04:10:13.870332Z",
          "shell.execute_reply.started": "2025-12-17T04:10:10.786633Z",
          "shell.execute_reply": "2025-12-17T04:10:13.869563Z"
        },
        "id": "Nh8toHrYyBul"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Extraction Function"
      ],
      "metadata": {
        "id": "hM4-who8yBul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_grammar_features(text):\n",
        "    features = {}\n",
        "\n",
        "    # ---------- BASIC STATS ----------\n",
        "    words = text.split()\n",
        "    word_count = len(words)\n",
        "    features[\"word_count\"] = word_count\n",
        "\n",
        "    doc = nlp(text)\n",
        "    sentences = list(doc.sents)\n",
        "    sentence_count = len(sentences)\n",
        "    features[\"sentence_count\"] = sentence_count\n",
        "\n",
        "    features[\"avg_sentence_length\"] = (\n",
        "        word_count / sentence_count if sentence_count > 0 else 0\n",
        "    )\n",
        "\n",
        "    # ---------- GRAMMAR ERROR FEATURES ----------\n",
        "    matches = tool.check(text)\n",
        "    error_count = len(matches)\n",
        "    features[\"grammar_error_count\"] = error_count\n",
        "\n",
        "    features[\"grammar_error_per_sentence\"] = (\n",
        "        error_count / sentence_count if sentence_count > 0 else 0\n",
        "    )\n",
        "\n",
        "    #  FIXED: Average grammar error span length (SAFE)\n",
        "    error_lengths = []\n",
        "    for m in matches:\n",
        "        try:\n",
        "            # Preferred: length of suggested replacement\n",
        "            if m.replacements:\n",
        "                error_lengths.append(len(m.replacements[0]))\n",
        "            else:\n",
        "                # Fallback: estimate from text slice\n",
        "                error_lengths.append(1)\n",
        "        except:\n",
        "            error_lengths.append(1)\n",
        "\n",
        "    features[\"avg_error_length\"] = (\n",
        "        np.mean(error_lengths) if error_lengths else 0\n",
        "    )\n",
        "\n",
        "    # ---------- SENTENCE COMPLETENESS ----------\n",
        "    incomplete_sentences = 0\n",
        "\n",
        "    for sent in sentences:\n",
        "        sent_doc = nlp(sent.text)\n",
        "        has_verb = any(tok.pos_ == \"VERB\" for tok in sent_doc)\n",
        "        has_subject = any(tok.dep_ in (\"nsubj\", \"nsubjpass\") for tok in sent_doc)\n",
        "\n",
        "        if not has_verb or not has_subject:\n",
        "            incomplete_sentences += 1\n",
        "\n",
        "    features[\"incomplete_sentence_ratio\"] = (\n",
        "        incomplete_sentences / sentence_count if sentence_count > 0 else 0\n",
        "    )\n",
        "\n",
        "    # ---------- POS RATIOS ----------\n",
        "    pos_counts = {}\n",
        "    for tok in doc:\n",
        "        pos_counts[tok.pos_] = pos_counts.get(tok.pos_, 0) + 1\n",
        "\n",
        "    for pos in [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"]:\n",
        "        features[f\"pos_{pos.lower()}_ratio\"] = (\n",
        "            pos_counts.get(pos, 0) / word_count if word_count > 0 else 0\n",
        "        )\n",
        "\n",
        "    # ---------- SYNTACTIC COMPLEXITY ----------\n",
        "    def tree_depth(token):\n",
        "        children = list(token.children)\n",
        "        if not children:\n",
        "            return 1\n",
        "        return 1 + max(tree_depth(child) for child in children)\n",
        "\n",
        "    depths = [tree_depth(sent.root) for sent in sentences] if sentences else [0]\n",
        "    features[\"avg_parse_tree_depth\"] = np.mean(depths)\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T04:10:13.871426Z",
          "iopub.execute_input": "2025-12-17T04:10:13.871887Z",
          "iopub.status.idle": "2025-12-17T04:10:13.880923Z",
          "shell.execute_reply.started": "2025-12-17T04:10:13.871868Z",
          "shell.execute_reply": "2025-12-17T04:10:13.880198Z"
        },
        "id": "vFbHOxD_yBul"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Feature Extraction on One Sample"
      ],
      "metadata": {
        "id": "oj5vys0WyBum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = train_df.iloc[0][\"transcript\"]\n",
        "extract_grammar_features(sample_text)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T04:10:13.881764Z",
          "iopub.execute_input": "2025-12-17T04:10:13.882092Z",
          "iopub.status.idle": "2025-12-17T04:10:14.754826Z",
          "shell.execute_reply.started": "2025-12-17T04:10:13.88207Z",
          "shell.execute_reply": "2025-12-17T04:10:14.75419Z"
        },
        "id": "vocszSycyBun"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract Features for Entire Training Set"
      ],
      "metadata": {
        "id": "VtPYSRLLyBun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "feature_rows = []\n",
        "\n",
        "for i in tqdm(range(len(train_df))):\n",
        "    text = train_df.loc[i, \"transcript\"]\n",
        "    feats =  extract_grammar_features(text)\n",
        "    feats[\"label\"]  =  train_df.loc[i, \"label\"]\n",
        "    feature_rows.append(feats)\n",
        "\n",
        "features_df =  pd.DataFrame(feature_rows)\n",
        "features_df.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-17T04:10:14.755711Z",
          "iopub.execute_input": "2025-12-17T04:10:14.756466Z"
        },
        "id": "A8WqFSZEyBun"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handle Missing"
      ],
      "metadata": {
        "id": "Nv32qZ44yBuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features_df =  features_df.fillna(0)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Vrunb53ByBuo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare Data  & Labels"
      ],
      "metadata": {
        "id": "RCV66beuyBuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preprare featrue target\n",
        "x =  features_df.drop(columns=[\"label\"])\n",
        "y = features_df[\"label\"]"
      ],
      "metadata": {
        "trusted": true,
        "id": "wpRSIG11yBuo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check shape"
      ],
      "metadata": {
        "id": "IJxXBPXmyBuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.shape, y.shape)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "FgOry915yBup"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-fold Cross-Validation Setup"
      ],
      "metadata": {
        "id": "p6cGhWwDyBup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "kf =  KFold(n_splits=5, shuffle=True, random_state=42)"
      ],
      "metadata": {
        "trusted": true,
        "id": "a66dAFOsyBup"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## install  xgboost"
      ],
      "metadata": {
        "id": "5zcygGfqyBup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "57RbvnJ3yBup"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train  with Cross Validation"
      ],
      "metadata": {
        "id": "iRTUO-IRyBup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from scipy.stats import pearsonr\n",
        "import numpy as np\n",
        "\n",
        "mae_scores = []\n",
        "rmse_scores = []\n",
        "pearson_scores = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(x)):\n",
        "    print(f\"\\nFold {fold+1}\")\n",
        "\n",
        "    X_train, X_val = x.iloc[train_idx], x.iloc[val_idx]\n",
        "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "    # ðŸ”¹ Scale target (FIT ONLY on train fold)\n",
        "    scaler = StandardScaler()\n",
        "    y_train_scaled = scaler.fit_transform(\n",
        "        y_train.values.reshape(-1, 1)\n",
        "    ).ravel()\n",
        "\n",
        "    # ðŸ”¹ Initialize model (fresh per fold)\n",
        "    xgb = XGBRegressor(\n",
        "        n_estimators=800,\n",
        "        max_depth=7,\n",
        "        learning_rate=0.03,\n",
        "        subsample=0.9,\n",
        "        colsample_bytree=0.9,\n",
        "        random_state=42,\n",
        "        objective=\"reg:squarederror\"\n",
        "    )\n",
        "\n",
        "    # ðŸ”¹ Train\n",
        "    xgb.fit(X_train, y_train_scaled)\n",
        "\n",
        "    # ðŸ”¹ Predict (scaled space)\n",
        "    preds_scaled = xgb.predict(X_val)\n",
        "\n",
        "    # ðŸ”¹ Inverse transform\n",
        "    preds = scaler.inverse_transform(\n",
        "        preds_scaled.reshape(-1, 1)\n",
        "    ).ravel()\n",
        "\n",
        "    # ðŸ”¹ Clip to valid grammar range\n",
        "    preds = np.clip(preds, 0, 5)\n",
        "\n",
        "    # ðŸ”¹ Metrics\n",
        "    mae = mean_absolute_error(y_val, preds)\n",
        "    rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
        "    pearson = pearsonr(y_val, preds)[0]\n",
        "\n",
        "    mae_scores.append(mae)\n",
        "    rmse_scores.append(rmse)\n",
        "    pearson_scores.append(pearson)\n",
        "\n",
        "    print(f\"MAE: {mae:.4f}\")\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "    print(f\"Pearson: {pearson:.4f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "8BgaJQokyBuq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n===== CROSS-VALIDATION RESULTS =====\")\n",
        "print(\"MAE scores:\", mae_scores)\n",
        "print(\"Mean MAE:\", np.mean(mae_scores))\n",
        "\n",
        "print(\"\\nRMSE scores:\", rmse_scores)\n",
        "print(\"Mean RMSE:\", np.mean(rmse_scores))\n",
        "\n",
        "print(\"\\nPearson scores:\", pearson_scores)\n",
        "print(\"Mean Pearson:\", np.mean(pearson_scores))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "FDgJSVLVyBuq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.scatter(y_val, preds, alpha=0.6)\n",
        "plt.xlabel(\"Actual Grammar Score\")\n",
        "plt.ylabel(\"Predicted Grammar Score\")\n",
        "plt.title(\"Prediction vs Actual\")\n",
        "plt.plot([0,5], [0,5], 'r--')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "7TY6h8gwyBuq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation\n",
        "\n",
        "The model was evaluated using 5-fold cross-validation. Since the competition\n",
        "leaderboard is based on RMSE and Pearson correlation, both metrics were computed\n",
        "for each fold. Target normalization was applied to improve ranking consistency,\n",
        "which significantly improved Pearson correlation.\n",
        "\n",
        "- Mean RMSE reflects absolute prediction accuracy\n",
        "- Mean Pearson correlation reflects ranking alignment with true grammar scores\n"
      ],
      "metadata": {
        "id": "jXFX2tCGyBuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "importance = pd.Series(\n",
        "    xgb.feature_importances_,\n",
        "    index=x.columns\n",
        ").sort_values(ascending=False)\n",
        "\n",
        "importance.head(10).plot(kind='barh')\n",
        "plt.title(\"Top Feature Importances\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "jBwyTsAjyBuq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## check test data"
      ],
      "metadata": {
        "id": "CJO7_SbDyBur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "kmWtJEFlyBur"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transcribe Test Audio"
      ],
      "metadata": {
        "id": "oRSeoIQ3yBur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "test_df[\"transcript\"] = \"\"\n",
        "\n",
        "TEST_AUDIO_DIR = os.path.join(shl_intern_hiring_assessment_2025_path, \"dataset\", \"audios\", \"test\")\n",
        "\n",
        "for i in tqdm(range(len(test_df))):\n",
        "    file_name = test_df.loc[i, \"filename\"]\n",
        "    file = file_name if file_name.lower().endswith('.wav') else file_name + '.wav'\n",
        "    audio_path = os.path.join(TEST_AUDIO_DIR, file)\n",
        "\n",
        "    text = transcribe_audio(audio_path)\n",
        "    test_df.loc[i, \"transcript\"] = clean_text(text)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "a8uLdWN6yBur"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save it"
      ],
      "metadata": {
        "id": "5DUpelHlyBur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.to_csv(\"test_with_transcripts.csv\", index=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "aNDPGmgJyBur"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract Grammer Feature For Test Set"
      ],
      "metadata": {
        "id": "wX_2fK1xyBus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_feature_rows = []\n",
        "\n",
        "for i in tqdm(range(len(test_df))):\n",
        "    text = test_df.loc[i, \"transcript\"]\n",
        "    feats = extract_grammar_features(text)\n",
        "    test_feature_rows.append(feats)\n",
        "\n",
        "X_test = pd.DataFrame(test_feature_rows)\n",
        "X_test = X_test.fillna(0)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "waWzuvNMyBus"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Final  Model  on Full training Data"
      ],
      "metadata": {
        "id": "B5eeg8MZyBus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_model = XGBRegressor(\n",
        "    n_estimators=400,\n",
        "    max_depth=5,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_train_full = features_df.drop(columns=[\"label\"])\n",
        "y_train_full = features_df[\"label\"]\n",
        "\n",
        "final_model.fit(X_train_full, y_train_full)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "-UdywsQTyBus"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict Grammer Scores For Test Set"
      ],
      "metadata": {
        "id": "ucnMLGxKyBus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = final_model.predict(X_test)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "RA4ItrneyBut"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To set valid range"
      ],
      "metadata": {
        "id": "RziELhYkyBut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = test_predictions.clip(0, 5)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "xfZ4pkffyBut"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Submission file"
      ],
      "metadata": {
        "id": "p3k1SiQdyBut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame({\n",
        "    \"filename\": test_df[\"filename\"],\n",
        "    \"label\": test_predictions.round(2)\n",
        "})\n",
        "\n",
        "submission.head()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "xxgIyvBPyBut"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "submission.to_csv(\"submission.csv\", index=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "3uicf6wEyBut"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Result Summary\n",
        "* Model: XGBoost Regressor\n",
        "* Evaluation metrics :  RMSEM , Pearson Correlation\n",
        "* Mean  Cross-Validation RMSE : 0.7333702368823092\n",
        "* Pearson Correlation:  0.34329159431492196"
      ],
      "metadata": {
        "id": "RPx-1MD-yBut"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion & Future Work\n",
        "\n",
        "This notebook presents a complete pipeline for spoken grammar scoring.\n",
        "Future improvements include:\n",
        "- Using larger ASR models\n",
        "- Incorporating language model perplexity\n",
        "- Model ensembling"
      ],
      "metadata": {
        "id": "BNfXWlVPyBuu"
      }
    }
  ]
}